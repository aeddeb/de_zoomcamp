{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 5 Homework\n",
    "### Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/01 19:55:56 WARN Utils: Your hostname, DESKTOP-FHCRPTH resolves to a loopback address: 127.0.1.1; using 172.29.215.187 instead (on interface eth0)\n",
      "22/03/01 19:55:56 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/aeddeb/spark/spark-3.0.3-bin-hadoop3.2/jars/spark-unsafe_2.12-3.0.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/03/01 19:55:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('week_5_hmw') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.3'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-03-01 00:01:16--  https://nyc-tlc.s3.amazonaws.com/trip+data/fhvhv_tripdata_2021-02.csv\n",
      "Resolving nyc-tlc.s3.amazonaws.com (nyc-tlc.s3.amazonaws.com)... 52.217.104.68\n",
      "Connecting to nyc-tlc.s3.amazonaws.com (nyc-tlc.s3.amazonaws.com)|52.217.104.68|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 733822658 (700M) [text/csv]\n",
      "Saving to: ‘fhvhv_tripdata_2021-02.csv’\n",
      "\n",
      "fhvhv_tripdata_2021 100%[===================>] 699.83M  52.6MB/s    in 12s     \n",
      "\n",
      "2022-03-01 00:01:28 (58.4 MB/s) - ‘fhvhv_tripdata_2021-02.csv’ saved [733822658/733822658]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://nyc-tlc.s3.amazonaws.com/trip+data/fhvhv_tripdata_2021-02.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType([\n",
    "    types.StructField('hvfhs_license_num', types.StringType(), True),\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True),\n",
    "    types.StructField('dropoff_datetime', types.TimestampType(), True),\n",
    "    types.StructField('PULocationID', types.IntegerType(), True),\n",
    "    types.StructField('DOLocationID', types.IntegerType(), True),\n",
    "    types.StructField('SR_Flag', types.StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv('fhvhv_tripdata_2021-02.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/01 19:58:11 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "22/03/01 19:58:11 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "22/03/01 19:58:11 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "22/03/01 19:58:11 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "22/03/01 19:58:11 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 63.33% for 12 writers\n",
      "22/03/01 19:58:11 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 58.46% for 13 writers\n",
      "22/03/01 19:58:11 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 54.29% for 14 writers\n",
      "22/03/01 19:58:11 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 50.67% for 15 writers\n",
      "22/03/01 19:58:11 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 47.50% for 16 writers\n",
      "22/03/01 19:58:15 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 50.67% for 15 writers\n",
      "22/03/01 19:58:15 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 54.29% for 14 writers\n",
      "22/03/01 19:58:15 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 58.46% for 13 writers\n",
      "22/03/01 19:58:15 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 63.33% for 12 writers\n",
      "22/03/01 19:58:15 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "22/03/01 19:58:15 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "22/03/01 19:58:15 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "22/03/01 19:58:15 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "22/03/01 19:58:15 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.parquet('fhvhv/2021/02/', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202M\t./fhvhv/2021/02\n",
      "202M\t./fhvhv/2021\n",
      "202M\t./fhvhv\n",
      "902M\t.\n"
     ]
    }
   ],
   "source": [
    "!du -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('fhvhv/2021/02')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(hvfhs_license_num='HV0003', dispatching_base_num='B02878', pickup_datetime=datetime.datetime(2021, 2, 3, 13, 34, 55), dropoff_datetime=datetime.datetime(2021, 2, 3, 13, 50, 32), PULocationID=170, DOLocationID=143, SR_Flag=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hvfhs_license_num',\n",
       " 'dispatching_base_num',\n",
       " 'pickup_datetime',\n",
       " 'dropoff_datetime',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'SR_Flag']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+------+\n",
      "|to_date(`pickup_datetime`)| count|\n",
      "+--------------------------+------+\n",
      "|                2021-02-15|367170|\n",
      "|                2021-02-02|302785|\n",
      "|                2021-02-26|500049|\n",
      "|                2021-02-21|399763|\n",
      "|                2021-02-05|478766|\n",
      "|                2021-02-10|403476|\n",
      "|                2021-02-01|176946|\n",
      "|                2021-02-06|470555|\n",
      "|                2021-02-19|459887|\n",
      "|                2021-02-20|497072|\n",
      "|                2021-02-08|398445|\n",
      "|                2021-02-09|398476|\n",
      "|                2021-02-11|428288|\n",
      "|                2021-02-17|415856|\n",
      "|                2021-02-25|436556|\n",
      "|                2021-02-27|509383|\n",
      "|                2021-02-24|412556|\n",
      "|                2021-02-18|372878|\n",
      "|                2021-02-14|460661|\n",
      "|                2021-02-12|469162|\n",
      "+--------------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(F.to_date(df.pickup_datetime)).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4\n",
    "Ref: https://sparkbyexamples.com/pyspark/pyspark-timestamp-difference-seconds-minutes-hours/#:~:text=Timestamp%20difference%20in%20PySpark%20can,the%20minute%20difference%20and%20finally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('DiffInSeconds',df.dropoff_datetime.cast(\"long\") - df.pickup_datetime.cast(\"long\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:=====================================>                  (16 + 8) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+-------------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|DiffInSeconds|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+-------------+\n",
      "|           HV0005|              B02510|2021-02-11 13:40:44|2021-02-12 10:39:44|         247|          41|   null|        75540|\n",
      "|           HV0004|              B02800|2021-02-17 15:54:53|2021-02-18 07:48:34|         242|         254|   null|        57221|\n",
      "|           HV0004|              B02800|2021-02-20 12:08:15|2021-02-21 00:22:14|         188|          55|   null|        44039|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.orderBy(df.DiffInSeconds.desc()).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|dispatching_base_num|  count|\n",
      "+--------------------+-------+\n",
      "|              B02876| 215693|\n",
      "|              B03136|   1741|\n",
      "|              B02877| 198938|\n",
      "|              B02869| 429720|\n",
      "|              B02883| 251617|\n",
      "|              B02835| 189031|\n",
      "|              B02884| 244963|\n",
      "|              B02880| 115716|\n",
      "|              B02878| 305185|\n",
      "|              B02836| 128978|\n",
      "|              B02872| 882689|\n",
      "|              B02512|  41043|\n",
      "|              B02867| 200530|\n",
      "|              B02866| 311089|\n",
      "|              B02871| 312364|\n",
      "|              B02889| 138762|\n",
      "|              B02844|   3502|\n",
      "|              B02510|3233664|\n",
      "|              B02888| 169167|\n",
      "|              B02682| 303255|\n",
      "+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('dispatching_base_num').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hvfhs_license_num',\n",
       " 'dispatching_base_num',\n",
       " 'pickup_datetime',\n",
       " 'dropoff_datetime',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'SR_Flag',\n",
       " 'DiffInSeconds']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+-----+\n",
      "|PULocationID|DOLocationID|count|\n",
      "+------------+------------+-----+\n",
      "|          76|          76|45041|\n",
      "|          26|          26|37329|\n",
      "|          39|          39|28026|\n",
      "|          61|          61|25976|\n",
      "|          14|          14|17934|\n",
      "|           7|           7|14688|\n",
      "|         129|         129|14688|\n",
      "|          42|          42|14481|\n",
      "|          37|          37|14424|\n",
      "|          89|          89|13976|\n",
      "|         216|         216|13716|\n",
      "|          35|          35|12829|\n",
      "|         132|         265|12542|\n",
      "|         188|          61|11814|\n",
      "|          95|          95|11548|\n",
      "|          36|          37|11491|\n",
      "|          37|          36|11487|\n",
      "|          61|         188|11462|\n",
      "|          61|         225|11342|\n",
      "|         188|         188|11308|\n",
      "+------------+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('PULocationID', 'DOLocationID').count().sort(F.desc('count')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = df.groupBy('PULocationID', 'DOLocationID').count().sort(F.desc('count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in zone_lookup\n",
    "zone = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .csv(\"taxi+_zone_lookup.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zone.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone = zone.select('LocationID', 'Zone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+-----+--------------------+--------------------+\n",
      "|PULocationID|DOLocationID|count|             PU_zone|             DO_zone|\n",
      "+------------+------------+-----+--------------------+--------------------+\n",
      "|          76|          76|45041|       East New York|       East New York|\n",
      "|          26|          26|37329|        Borough Park|        Borough Park|\n",
      "|          39|          39|28026|            Canarsie|            Canarsie|\n",
      "|          61|          61|25976| Crown Heights North| Crown Heights North|\n",
      "|          14|          14|17934|           Bay Ridge|           Bay Ridge|\n",
      "|           7|           7|14688|             Astoria|             Astoria|\n",
      "|         129|         129|14688|     Jackson Heights|     Jackson Heights|\n",
      "|          42|          42|14481|Central Harlem North|Central Harlem North|\n",
      "|          37|          37|14424|      Bushwick South|      Bushwick South|\n",
      "|          89|          89|13976|Flatbush/Ditmas Park|Flatbush/Ditmas Park|\n",
      "|         216|         216|13716|    South Ozone Park|    South Ozone Park|\n",
      "|          35|          35|12829|         Brownsville|         Brownsville|\n",
      "|         132|         265|12542|         JFK Airport|                  NA|\n",
      "|         188|          61|11814|Prospect-Lefferts...| Crown Heights North|\n",
      "|          95|          95|11548|        Forest Hills|        Forest Hills|\n",
      "|          36|          37|11491|      Bushwick North|      Bushwick South|\n",
      "|          37|          36|11487|      Bushwick South|      Bushwick North|\n",
      "|          61|         188|11462| Crown Heights North|Prospect-Lefferts...|\n",
      "|          61|         225|11342| Crown Heights North|  Stuyvesant Heights|\n",
      "|         188|         188|11308|Prospect-Lefferts...|Prospect-Lefferts...|\n",
      "+------------+------------+-----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# join\n",
    "result.join(zone, result.PULocationID == zone.LocationID, 'inner') \\\n",
    "    .drop('LocationID') \\\n",
    "    .withColumnRenamed('Zone', 'PU_zone') \\\n",
    "    .join(zone, result.DOLocationID == zone.LocationID, 'inner') \\\n",
    "    .drop('LocationID') \\\n",
    "    .withColumnRenamed('Zone', 'DO_zone') \\\n",
    "    .sort(F.desc('count')) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
